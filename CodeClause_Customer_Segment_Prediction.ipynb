{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnLA/MOcyjpGp/o2n/0f9e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YossefEFM/CodeClause_Customer_Segment_Prediction/blob/main/CodeClause_Customer_Segment_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mM1Bfwrp0b45"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import  confusion_matrix,classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data & Explore it"
      ],
      "metadata": {
        "id": "dzO0X57r03wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "Data = pd.read_csv(\"/content/Customer Segment.csv\")\n",
        "# Explore Dataset\n",
        "print(Data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA5n52Zp0iod",
        "outputId": "b9db0535-3aa3-4728-9e66-a08e4a6869c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2627 entries, 0 to 2626\n",
            "Data columns (total 11 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   ID               2627 non-null   int64  \n",
            " 1   Gender           2627 non-null   object \n",
            " 2   Ever_Married     2577 non-null   object \n",
            " 3   Age              2627 non-null   int64  \n",
            " 4   Graduated        2603 non-null   object \n",
            " 5   Profession       2589 non-null   object \n",
            " 6   Work_Experience  2358 non-null   float64\n",
            " 7   Spending_Score   2627 non-null   object \n",
            " 8   Family_Size      2514 non-null   float64\n",
            " 9   Var_1            2595 non-null   object \n",
            " 10  Segmentation     2627 non-null   object \n",
            "dtypes: float64(2), int64(2), object(7)\n",
            "memory usage: 225.9+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "RRiOeFMG0w7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 1/ Cleaning the data from Missing values\n",
        "print(\"\\n Missing values : \")\n",
        "print(Data.isna().sum())\n",
        "Data.dropna(axis =0 , inplace=True)\n",
        "\n",
        "# 2/ Check duplicates\n",
        "print(\"\\n Duplicates  :\")\n",
        "print(Data.duplicated().sum())\n",
        "\n",
        "# 3/ Encoding Data\n",
        "labelencoder=LabelEncoder()\n",
        "Data['Segmentation'] = labelencoder.fit_transform(Data['Segmentation'])\n",
        "Data['Gender'] = labelencoder.fit_transform(Data['Gender'])\n",
        "Data['Ever_Married'] = labelencoder.fit_transform(Data['Ever_Married'])\n",
        "Data['Spending_Score'] = labelencoder.fit_transform(Data['Spending_Score'])\n",
        "Data['Graduated'] = labelencoder.fit_transform(Data['Graduated'])\n",
        "Data['Var_1'] = labelencoder.fit_transform(Data['Var_1'])\n",
        "Data['Profession'] = labelencoder.fit_transform(Data['Profession'])\n",
        "\n",
        "# 4/ Removing Unwanted columns\n",
        "print(\"\\n\",Data.corr(),\"\\n\")\n",
        "Data.drop(columns= \"ID\", inplace=True)\n",
        "\n",
        "print(Data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbAGtCpy0nAY",
        "outputId": "098a62d6-ea37-4d49-f0b9-08c0cf37b9bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Missing values : \n",
            "ID                   0\n",
            "Gender               0\n",
            "Ever_Married        50\n",
            "Age                  0\n",
            "Graduated           24\n",
            "Profession          38\n",
            "Work_Experience    269\n",
            "Spending_Score       0\n",
            "Family_Size        113\n",
            "Var_1               32\n",
            "Segmentation         0\n",
            "dtype: int64\n",
            "\n",
            " Duplicates  :\n",
            "0\n",
            "\n",
            "                        ID    Gender  Ever_Married       Age  Graduated  \\\n",
            "ID               1.000000 -0.002259      0.045929 -0.005315  -0.005888   \n",
            "Gender          -0.002259  1.000000      0.104014 -0.010489  -0.069970   \n",
            "Ever_Married     0.045929  0.104014      1.000000  0.579491   0.204191   \n",
            "Age             -0.005315 -0.010489      0.579491  1.000000   0.247321   \n",
            "Graduated       -0.005888 -0.069970      0.204191  0.247321   1.000000   \n",
            "Profession       0.002519  0.050111     -0.100911  0.062906  -0.329968   \n",
            "Work_Experience -0.018560 -0.064181     -0.119011 -0.185417   0.056528   \n",
            "Spending_Score  -0.019156 -0.051201     -0.615536 -0.294270  -0.153499   \n",
            "Family_Size      0.013992  0.082164     -0.097469 -0.284672  -0.277731   \n",
            "Var_1           -0.023396  0.016592      0.080307  0.191262   0.161593   \n",
            "Segmentation    -0.014410 -0.009789     -0.063428 -0.063000  -0.047221   \n",
            "\n",
            "                 Profession  Work_Experience  Spending_Score  Family_Size  \\\n",
            "ID                 0.002519        -0.018560       -0.019156     0.013992   \n",
            "Gender             0.050111        -0.064181       -0.051201     0.082164   \n",
            "Ever_Married      -0.100911        -0.119011       -0.615536    -0.097469   \n",
            "Age                0.062906        -0.185417       -0.294270    -0.284672   \n",
            "Graduated         -0.329968         0.056528       -0.153499    -0.277731   \n",
            "Profession         1.000000        -0.037441        0.214221     0.114758   \n",
            "Work_Experience   -0.037441         1.000000        0.081768    -0.074567   \n",
            "Spending_Score     0.214221         0.081768        1.000000    -0.101380   \n",
            "Family_Size        0.114758        -0.074567       -0.101380     1.000000   \n",
            "Var_1             -0.013258         0.018414       -0.020211    -0.189388   \n",
            "Segmentation       0.071384        -0.024177        0.036361     0.015854   \n",
            "\n",
            "                    Var_1  Segmentation  \n",
            "ID              -0.023396     -0.014410  \n",
            "Gender           0.016592     -0.009789  \n",
            "Ever_Married     0.080307     -0.063428  \n",
            "Age              0.191262     -0.063000  \n",
            "Graduated        0.161593     -0.047221  \n",
            "Profession      -0.013258      0.071384  \n",
            "Work_Experience  0.018414     -0.024177  \n",
            "Spending_Score  -0.020211      0.036361  \n",
            "Family_Size     -0.189388      0.015854  \n",
            "Var_1            1.000000      0.006476  \n",
            "Segmentation     0.006476      1.000000   \n",
            "\n",
            "   Gender  Ever_Married  Age  Graduated  Profession  Work_Experience  \\\n",
            "0       0             1   36          1           2              0.0   \n",
            "1       1             1   37          1           5              8.0   \n",
            "3       1             1   59          0           4             11.0   \n",
            "5       1             1   47          1           1              0.0   \n",
            "6       1             1   61          1           1              5.0   \n",
            "\n",
            "   Spending_Score  Family_Size  Var_1  Segmentation  \n",
            "0               2          1.0      5             1  \n",
            "1               0          4.0      5             0  \n",
            "3               1          2.0      5             1  \n",
            "5               1          5.0      3             2  \n",
            "6               2          3.0      5             3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "3jJnrXmt1Cm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 1/ Splitting data\n",
        "# X --> features , Y --> Label\n",
        "\n",
        "X = Data.drop(columns='Segmentation')\n",
        "Y = Data.Segmentation\n",
        "\n",
        "  # ** Split Features & Label to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20,  shuffle= True ,random_state= 1 )\n"
      ],
      "metadata": {
        "id": "uSNyvVhR0qgc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # 2/ Calling Logistic regression\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "  # 3/ Train data\n",
        "log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToHbbxhU1Jcf",
        "outputId": "e50b50c0-4c66-4b17-b509-bb0136ff1a44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  # 4/ Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        " # Confisuion Matrix\n",
        "#print(confusion_matrix(y_test, y_pred))\n",
        "# 5/ Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print('report:', report, sep='\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTos4c1-0s0s",
        "outputId": "1e2e8c92-d1a2-4894-a144-426d8c1f8d50"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.55      0.44       152\n",
            "           1       0.23      0.06      0.09        85\n",
            "           2       0.38      0.17      0.24        70\n",
            "           3       0.41      0.50      0.45       124\n",
            "\n",
            "    accuracy                           0.38       431\n",
            "   macro avg       0.35      0.32      0.30       431\n",
            "weighted avg       0.35      0.38      0.34       431\n",
            "\n"
          ]
        }
      ]
    }
  ]
}